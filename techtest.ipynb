{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.datasets import mnist\nimport random\n\n# Bayesian model imports\nimport pystan\nfrom scipy.special import expit as logistic_sigmoid\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Exploratory Data Analysis (EDA)\nprint(\"Number of training samples:\", x_train.shape[0])\nprint(\"Number of test samples:\", x_test.shape[0])\nprint(\"Shape of an image:\", x_train[0].shape)\n\n# Visualize some samples from the dataset\nfig, axes = plt.subplots(1, 10, figsize=(10, 1))\nfor i in range(10):\n    axes[i].imshow(x_train[i], cmap='gray')\n    axes[i].axis('off')\nplt.show()\n\n# Data preprocessing\nx_train = x_train.reshape(-1, 28*28) / 255.0\nx_test = x_test.reshape(-1, 28*28) / 255.0\n\n# Split validation set from training set\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)\nx_test = scaler.transform(x_test)\n\n# Train three different models\n\n# Logistic Regression\nlog_reg = LogisticRegression(max_iter=1000)\nlog_reg.fit(x_train, y_train)\nlog_reg_val_pred = log_reg.predict(x_val)\nlog_reg_accuracy = accuracy_score(y_val, log_reg_val_pred)\n\n# Random Forest\nrf_clf = RandomForestClassifier(n_estimators=100)\nrf_clf.fit(x_train, y_train)\nrf_val_pred = rf_clf.predict(x_val)\nrf_accuracy = accuracy_score(y_val, rf_val_pred)\n\n# Neural Network\nmlp_clf = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=20)\nmlp_clf.fit(x_train, y_train)\nmlp_val_pred = mlp_clf.predict(x_val)\nmlp_accuracy = accuracy_score(y_val, mlp_val_pred)\n\n# Bayesian Logistic Regression Model using PyStan\nstan_model_code = \"\"\"\ndata {\n  int<lower=0> N;  // number of observations\n  int<lower=0> K;  // number of predictors\n  matrix[N, K] X;  // predictor matrix\n  int<lower=0, upper=1> y[N];  // outcome variable\n}\nparameters {\n  vector[K] beta;  // coefficients for predictors\n}\nmodel {\n  y ~ bernoulli_logit(X * beta);  // likelihood\n  beta ~ normal(0, 1);  // prior\n}\n\"\"\"\n\nstan_data = {\n    'N': x_train.shape[0],\n    'K': x_train.shape[1],\n    'X': x_train,\n    'y': y_train\n}\n\nstan_model = pystan.StanModel(model_code=stan_model_code)\nstan_fit = stan_model.sampling(data=stan_data, iter=1000, chains=4)\n\n# Extract posterior means for beta\nbeta_post = stan_fit.extract()['beta']\nbeta_mean = np.mean(beta_post, axis=0)\n\n# Make predictions using the Bayesian model\ndef bayesian_predict(X, beta_mean):\n    return logistic_sigmoid(np.dot(X, beta_mean))\n\nbayesian_val_pred = bayesian_predict(x_val, beta_mean)\nbayesian_val_pred_class = (bayesian_val_pred > 0.5).astype(int)\nbayesian_accuracy = accuracy_score(y_val, bayesian_val_pred_class)\n\n# Output validation accuracy for each model\nprint(f\"Logistic Regression Accuracy: {log_reg_accuracy}\")\nprint(f\"Random Forest Accuracy: {rf_accuracy}\")\nprint(f\"Neural Network Accuracy: {mlp_accuracy}\")\nprint(f\"Bayesian Model Accuracy: {bayesian_accuracy}\")\n\n# Define a function to evaluate a chosen model\ndef evaluate_model(model_name, n_samples):\n    model_dict = {\n        \"logistic_regression\": log_reg,\n        \"random_forest\": rf_clf,\n        \"neural_network\": mlp_clf,\n        \"bayesian_model\": (bayesian_predict, beta_mean)\n    }\n    \n    if model_name not in model_dict:\n        print(\"Invalid model name.\")\n        return\n    \n    indices = random.sample(range(len(x_test)), n_samples)\n    x_sample = x_test[indices]\n    y_sample = y_test[indices]\n    \n    if model_name == \"bayesian_model\":\n        y_pred = model_dict[model_name][0](x_sample, model_dict[model_name][1])\n        y_pred = (y_pred > 0.5).astype(int)\n    else:\n        model = model_dict[model_name]\n        y_pred = model.predict(x_sample)\n    \n    results = pd.DataFrame({\n        \"Actual\": y_sample,\n        \"Predicted\": y_pred\n    })\n    \n    accuracy_per_digit = results.groupby(\"Actual\").apply(lambda x: (x[\"Actual\"] == x[\"Predicted\"]).mean())\n    \n    print(f\"Accuracy per digit for {model_name}:\")\n    print(accuracy_per_digit)\n    \n    return results\n\n# Interactive program for user to evaluate models\ndef interactive_evaluation():\n    while True:\n        model_name = input(\"Choose a model (logistic_regression, random_forest, neural_network, bayesian_model): \")\n        n_samples = int(input(\"Enter the number of samples to evaluate: \"))\n        evaluate_model(model_name, n_samples)\n        \n        cont = input(\"Do you want to continue? (yes/no): \")\n        if cont.lower() != \"yes\":\n            break\n\n# Run the interactive evaluation\ninteractive_evaluation()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}